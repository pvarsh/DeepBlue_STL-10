\documentclass{article} % For LaTeX2e
\usepackage{nips14submit_e,times,cite}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Assignment 2: Classifying STL-10 with a deep feed-forward convolutional neural net}


\author{
Priyank Bhatia \\
New York University \\
Center for Urban Science + Progress \\
1 MetroTech Center, 19th Floor \\
Brooklyn, NY 11201 \\
\texttt{pb1672@nyu.edu} \\
\AND
Emil Christensen \\
New York University \\
Center for Urban Science + Progress \\
1 MetroTech Center, 19th Floor \\
Brooklyn, NY 11201 \\
\texttt{erc399@nyu.edu} \\
\And
Peter Varshavsky \\
New York University \\
Center for Urban Science + Progress \\
1 MetroTech Center, 19th Floor \\
Brooklyn, NY 11201 \\
\texttt{pv629@nyu.edu} \\
}


% Some example code:

%\begin{center}
%   \url{URLs go here}
%\end{center}

%\section{A section!}
%\subsection{A subsection!}

%\label{sub_1} - label a section
%\ref{sub_1} - reference a labeled section

%\footnote{Sample of a footnote}

% Figures:
% \begin{figure}[h]
% \begin{center}
% \framebox[4.0in]{$\;$}
% \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
% \end{center}
% \caption{Sample figure caption.}
% \end{figure}

% Tables:
% \begin{table}[t]
% \caption{Sample table title}
% \label{sample-table}
% \begin{center}
% \begin{tabular}{ll}
% \multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
% \\ \hline \\
% Dendrite         &Input terminal \\
% Axon             &Output terminal \\
% Soma             &Cell body (contains cell nucleus) \\
% \end{tabular}
% \end{center}
% \end{table}

% Lists (can be recursive):
% \begin{itemize}
% \item Item 1
% \item Item 2
% \item Item 3
% \end{itemize}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\nipsfinalcopy % Uncomment for camera-ready version


\begin{document}

\maketitle


\begin{abstract}
The abstract paragraph goes here!
\end{abstract}

\section{Data}
\label{data}
Designed to study the use of unlabeled data for image classification, the STL-10 dataset \cite{coates2011analysis} consists of three sets of 96x96 pixel RGB color images: 5000 labeled training images, 8000 labeled test images, and 100000 unlabeled images. Each image belongs to one of 10 categories. In this submission we only utilize the labeled portion of STL-10 and attempt to improve classification quality by image augmentation and depth of network.

\section{Architecture}
\label{arc}
Two versions of a feed-forward convolutional neural net architectures were compared. The first architecture was an implementation of the baseline model by Christian Puhrsch with one convolutional layer of 23 7x7 pixel learned filters with a stepwise of 2, ReLU nonlinearity, 2 sq pixel pooling with step size 2, 50\% dropout, a 50-node fully connected layer, pointwise ReLU and LogSoftMax and, and a negative log likelihood criterion.
The second model used the same basic architecture but consisted of two convolutional layers of 200 5x5 pixel filters each, and a 400-neuron fully connected linear layer. Spatial pooling, rectified linear unit nonlinearities and dropout were applied as in the first network.

\section{Preprocessing and augmentation}
\label{preproc}
The original 5000 training images were split into training and validation sets of sizes 4500 and 500. To ameliorate the small training size and improve feature invariance the 4500 training images were cloned twice. The first cloned set was flipped horizontally, and the second was rotated counter-clockwise by 0.35 radians. This yielded an augmented training set of 13500 images. We further attempted to augment the training set using contrast HSV color space adjustments similar to contrast2 in ~\cite{DosovitskiySRB14}, small random translations and rotations, but ran into training convergence issues, possibly due to coding errors. Augmented data were converted to YUV color space. Training images were globally normalized. Validation and test images were globally normalized using training mean and standard deviation. All images were further locally normalized and given a 2-pixel zero pad.

\section{Learning Techniques}
\label{learn}

\section{Training Procedure}
\label{train}
Forward and back prop. Learning rate 0.1. Batch size 8. Rate annealing 0.001 (check), metaparameters determined with validation. Batch GD, batch to increase efficiency, while ...

\section{Results}
\label{res}
\begin{center}
  \begin{tabular}{ | l | l | l | l |}
  \hline
  \multicolumn{4}{| c |}{Model 1 - One convolutional layer} \\ \hline
                           & train error & validation error & test error \\ \hline
  train with validation    & ...         & ...              & ...        \\ \hline
  train without validation & ...         & ...        & ...      \\ \hline
  
  \end{tabular}
\end{center}

\begin{center}
  \begin{tabular}{ | l | l | l | l |}
  \hline
  \multicolumn{4}{| c |}{Model 2 - Two convolutional layers} \\ \hline
                           & train error & validation error & test error \\ \hline
  train with validation    & 87.82\%     & 60.0\%           & 61.69\%    \\ \hline
  train without validation & ...         & N/A        & ...      \\ \hline
  
  \end{tabular}
\end{center}


\bibliography{citations}{}
\bibliographystyle{plain}

\end{document}

